{"cells":[{"cell_type":"code","source":["# Comment the following lines if you're not in colab:\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# If you're in colab, cd to your own working directory here:\n","%cd ..//..//content//drive//MyDrive//Colab-Notebooks//HY-673-Tutorials//Tutorial-2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eEBQeWtdFpne","executionInfo":{"status":"ok","timestamp":1707830358478,"user_tz":-120,"elapsed":19020,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"19f83448-e0c8-4758-b716-ff16a1b212d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab-Notebooks/HY-673-Tutorials/Tutorial-2\n"]}]},{"cell_type":"markdown","metadata":{"id":"4JDjL7pZEzOM"},"source":["## <u>From CSV Files to Datasets</u>\n","\n","We will prepare a PyTorch Dataset for a very simple classification problem, i.e., distinguishing between different types of Iris flowers.\n","\n","The dataset is stored as a CSV file in the `\"data/iris_data.csv\"` folder. In order to prepare a PyTorch Dataset, we will need to go through the following steps:\n","- Load the file with Pandas and perform any preprocessing that's necessary\n","- Divide the data between training, test (and sometimes validation)\n","- Create a PyTorch Dataset by extending the `torch.utils.data.Dataset` class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poLAcwl3EzOR"},"outputs":[],"source":["import os\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"eLgjNUfeEzOT"},"source":["## <u>Pandas</u>\n","\n","A useful tool to preprocess your data in Python is Pandas. You can use it to manipulate data files, such as, CSV, XLSX, and others, which are read as `DataFrame` objects.\n","\n","In order to read such data, you should specify either their absolute or relative path. I personally recommend using relative paths, since they make the code easier to run on other people's machines.\n","\n","You can specify the path to a file simply as a string (e.g., `\"data//iris_data.csv\"`). However, it is better to use the `os.path.join()` function to make your program work across many different platforms:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxJe4LPdEzOT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830359227,"user_tz":-120,"elapsed":2,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"8b445717-b8e6-47e8-bdc0-74620893a112"},"outputs":[{"output_type":"stream","name":"stdout","text":["Local Path: data/iris_data.csv\n"]}],"source":["datapath = os.path.join('data', 'iris_data.csv')\n","print(\"Local Path:\", datapath)"]},{"cell_type":"markdown","metadata":{"id":"PHmyuso7EzOU"},"source":["Now, let's read the CSV file `\"data/iris_data.csv\"` with Pandas. We can do that using the `pd.read_csv()` function. Next, let's visualize its contents:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R30dm4wlEzOU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830359660,"user_tz":-120,"elapsed":435,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"db8e8714-c066-4e4d-dd85-30246e50885f"},"outputs":[{"output_type":"stream","name":"stdout","text":["     sepal_length  sepal_width  petal_length  petal_width           label\n","0             5.1          3.5           1.4          0.2     Iris-setosa\n","1             4.9          3.0           1.4          0.2     Iris-setosa\n","2             4.7          3.2           1.3          0.2     Iris-setosa\n","3             4.6          3.1           1.5          0.2     Iris-setosa\n","4             5.0          3.6           1.4          0.2     Iris-setosa\n","..            ...          ...           ...          ...             ...\n","145           6.7          3.0           5.2          2.3  Iris-virginica\n","146           6.3          2.5           5.0          1.9  Iris-virginica\n","147           6.5          3.0           5.2          2.0  Iris-virginica\n","148           6.2          3.4           5.4          2.3  Iris-virginica\n","149           5.9          3.0           5.1          1.8  Iris-virginica\n","\n","[150 rows x 5 columns]\n"]}],"source":["df = pd.read_csv(datapath)\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"1mMnODVzEzOV"},"source":["Individual columns of a `DataFrame` are called `Series` objects. Typically the operations that you can perform on DataFrames and Series are similar, but it is a good idea to always check the documentation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"goWmIPDQEzOW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830359660,"user_tz":-120,"elapsed":9,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"12dec693-24c6-451f-a19a-990ba2f645b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Object is of type: <class 'pandas.core.frame.DataFrame'>\n","\n","The columns are of type: <class 'pandas.core.series.Series'>\n","\n","Labels are of type: <class 'str'>\n"]}],"source":["print(f\"Object is of type: {type(df)}\")\n","print(f\"\\nThe columns are of type: {type(df['sepal_length'])}\")\n","print(f\"\\nLabels are of type: {type(df['label'][0])}\")"]},{"cell_type":"markdown","metadata":{"id":"UPt9JYL6EzOW"},"source":["You can see that the `label` column of the Iris dataset contains strings. However, in order to classify data, we can encode the labels into integer values: a string-to-integer mapping.\n","\n","One simple way to do that is to create a dictionary, with keys and values that represent this mapping. Let's start by seeing what are all the possible labels for our data. We can do that by using the `unique()` method of Pandas Series:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTgeDqYAEzOW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830359660,"user_tz":-120,"elapsed":7,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"a7af7c56-cd5c-4382-9cbc-00d2850be171"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"]}],"source":["print(df['label'].unique())"]},{"cell_type":"markdown","metadata":{"id":"wmyX87ZNEzOW"},"source":["We can see that there are 3 (unique) classes in total. We can now define our dictionary for the Iris labels as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnHguD3wEzOW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830359661,"user_tz":-120,"elapsed":7,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"45583291-dc09-4d12-fce0-166d9ce59be4"},"outputs":[{"output_type":"stream","name":"stdout","text":["The dictionary: {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n","Class that corresponds to 'Iris-versicolor' is 1.\n"]}],"source":["label_dict = {\n","    'Iris-setosa': 0,\n","    'Iris-versicolor': 1,\n","    'Iris-virginica': 2\n","}\n","print(f\"The dictionary: {label_dict}\")\n","print(f\"Class that corresponds to 'Iris-versicolor' is {label_dict['Iris-versicolor']}.\")"]},{"cell_type":"markdown","metadata":{"id":"Q0DGZ-2CEzOX"},"source":["However, this is only workable for simple cases, because such method is not scalable, and prone to errors. What if, say, we have 100 classes, or we accidentally misspell `Iris-versicolor`? The `enumerate()` built-in function comes in handy here:"]},{"cell_type":"code","source":["for i, j in enumerate(['ab', 'cdef', 'gh', 'i', 'jkl']):\n","  print(f\"i = {i}, j = {j}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwLfp94FZ1WX","executionInfo":{"status":"ok","timestamp":1707830359661,"user_tz":-120,"elapsed":6,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"8b405080-76ed-4a7e-d426-08273cb86c2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["i = 0, j = ab\n","i = 1, j = cdef\n","i = 2, j = gh\n","i = 3, j = i\n","i = 4, j = jkl\n"]}]},{"cell_type":"markdown","source":["This will help us create the same dictionary but in a more automated way, i.e., without the need to hardcode it. We can do so with this beautiful one-liner:"],"metadata":{"id":"gFQC5gqZZ38I"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cY2Ok2IEzOX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830359661,"user_tz":-120,"elapsed":5,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"e027c4dc-ea15-44a7-fc67-4311e8758e92"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n"]}],"source":["label_dict = {label: i for i, label in enumerate(df['label'].unique())}\n","print(label_dict)"]},{"cell_type":"markdown","metadata":{"id":"-kN-icEaEzOX"},"source":["Now we just need to \"apply\" our dictionary to the original dataset labels in order to change them from string to integer. We can do that by simply using the `map()` built-in function (there are also other ways e.g., using `apply()` with a lambda expression). Remember, since both `Series` and `DataFrame` objects are containers, the modifications will affect the original `df` object as well:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppfXv61XEzOX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830359661,"user_tz":-120,"elapsed":4,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"e404ce76-8da4-4b19-b036-8ee3b7b195cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["New dataset with string labels replaced by integers:\n","\n","     sepal_length  sepal_width  petal_length  petal_width  label\n","0             5.1          3.5           1.4          0.2      0\n","1             4.9          3.0           1.4          0.2      0\n","2             4.7          3.2           1.3          0.2      0\n","3             4.6          3.1           1.5          0.2      0\n","4             5.0          3.6           1.4          0.2      0\n","..            ...          ...           ...          ...    ...\n","145           6.7          3.0           5.2          2.3      2\n","146           6.3          2.5           5.0          1.9      2\n","147           6.5          3.0           5.2          2.0      2\n","148           6.2          3.4           5.4          2.3      2\n","149           5.9          3.0           5.1          1.8      2\n","\n","[150 rows x 5 columns]\n"]}],"source":["df['label'] = df['label'].map(label_dict)\n","# Same as:\n","# df['label'] = df['label'].apply(lambda x: label_dict[x])\n","print(f\"New dataset with string labels replaced by integers:\\n\\n{df}\")"]},{"cell_type":"markdown","metadata":{"id":"ARwemNsVEzOY"},"source":["Now, we can save our updated DataFrame if we want using the `to_csv()` method:\n","\n","(In most cases you want to set the parameter `index` to `False`. Otherwise, the CSV file will contain an extra column with the index values of the DataFrame. I have no idea why this parameter is set to `True` by default.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-KvOz0kEzOY"},"outputs":[],"source":["datapath_new = os.path.join(\"data\", \"iris_new.csv\")\n","df.to_csv(datapath_new, index=False)"]},{"cell_type":"markdown","metadata":{"id":"lLSBF9YVEzOY"},"source":["## <u>Train/Test Split</u>\n","\n","If you have any experience with machine learning, you know that you should split your dataset into a *train set*, a *test set*, and, optionally, a *validation set*. Here, we will just split it into a train and a test set. The model must never be trained on the test set; it should never \"see\" any part of it in any way.\n","\n","First of all, let's convert our DataFrame to a Numpy array, and separate the input data from the ground-truth labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6b-NO92kEzOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830359979,"user_tz":-120,"elapsed":2,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"46044b09-8a12-4a66-a100-17038e27412a"},"outputs":[{"output_type":"stream","name":"stdout","text":["total number of data points  = 150\n","dimensionality of each point = 4\n","\n","10 inputs:\n"," [[5.1 3.5 1.4 0.2]\n"," [4.9 3.  1.4 0.2]\n"," [4.7 3.2 1.3 0.2]\n"," [4.6 3.1 1.5 0.2]\n"," [5.  3.6 1.4 0.2]\n"," [5.4 3.9 1.7 0.4]\n"," [4.6 3.4 1.4 0.3]\n"," [5.  3.4 1.5 0.2]\n"," [4.4 2.9 1.4 0.2]\n"," [4.9 3.1 1.5 0.1]]\n","10 ground truths:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}],"source":["all_data = df.to_numpy()\n","x_data = all_data[:, :-1]  # inputs\n","y_data = all_data[:, -1]   # ground truth (label)\n","\n","print(f\"total number of data points  = {x_data.shape[0]}\")\n","print(f\"dimensionality of each point = {x_data.shape[1]}\")\n","\n","print(f\"\\n10 inputs:\\n {x_data[:10]}\\n10 ground truths:\\n {y_data[:10]}\")"]},{"cell_type":"markdown","metadata":{"id":"v5swNua2EzOY"},"source":["Now, time to actually split the dataset into training and test data. We'll do a 70â€”30 split, meaning that 70% of our data will be used for training and 30% for testing.\n","\n","We can do that by using the `train_test_split()` function of Scikit-Learn. You can find the documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","\n","I have two main recommendations for you regarding this function:\n","- Make the split reproducible by setting a `random_state` (e.g., 42)\n","- Use the `stratify` option with respect to the labels: this will ensure that the training and test data have roughly the same class distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amcRjdnnEzOY"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(\n","    x_data, y_data, test_size=1/3, random_state=42, stratify=y_data\n",")"]},{"cell_type":"markdown","metadata":{"id":"GK1uPY4LEzOY"},"source":["Let's print some information to check if everything has probably gone smoothly:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63qR02csEzOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830361276,"user_tz":-120,"elapsed":2,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"c76edefe-1879-438b-aa49-48d1825b1de2"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape: (100, 4), y_train shape: (100,)\n","x_test shape:  (50, 4),  y_test shape:  (50,)\n","\n","x_train[:3] = \n","[[6.3 3.4 5.6 2.4]\n"," [5.1 3.5 1.4 0.3]\n"," [5.8 2.7 5.1 1.9]] \n","y_train[:3]:\n"," [2. 0. 2.]\n","\n","x_test[:3] = \n","[[6.3 2.8 5.1 1.5]\n"," [6.3 3.3 4.7 1.6]\n"," [5.  3.4 1.5 0.2]] \n","y_test[:3]:\n"," [2. 1. 0.]\n"]}],"source":["print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n","print(f\"x_test shape:  {x_test.shape},  y_test shape:  {y_test.shape}\")\n","print(f\"\\nx_train[:3] = \\n{x_train[:3]} \\ny_train[:3]:\\n {y_train[:3]}\")\n","print(f\"\\nx_test[:3] = \\n{x_test[:3]} \\ny_test[:3]:\\n {y_test[:3]}\")"]},{"cell_type":"markdown","metadata":{"id":"GqG23N1HEzOY"},"source":["## <u>Pickle</u>\n","\n","In cases like this, preprocessing the data does not take much time computation-wise. So, perhaps there is no problem preprocessing the data every time we run the entire code. But what if we cannot afford that, because, say, it is too expensive to do so every single time?\n","\n","In Python, we can use the in-built Pickle module to save and load groups of variables in a compressed format.\n","\n","Important: Use Pickle only to save and load variables within your working space! The adopted format varies depeding on the version, so, it is bad practice to use Pickle files to share data with others."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIBVQuc9EzOY"},"outputs":[],"source":["import pickle\n","\n","pickledatapath = os.path.join('data', 'iris_data.pkl')\n","\n","with open(pickledatapath, 'wb') as f:\n","    pickle.dump([x_train, x_test, y_train, y_test], f)"]},{"cell_type":"markdown","metadata":{"id":"OYOwnptsEzOZ"},"source":["## <u>Pytorch Datasets and DataLoaders</u>\n","\n","The best practice to feed data to your machine learning model, is by storing them as a dataset and subsequently to divide them in batches with a `DataLoader`.\n","\n","In order to define a Dataset in Pytorch, you need to extend the `Dataset` class and specify 3 methods:\n","- `__init__()`: the class constructor\n","- `__len__()`: the method that returns the dataset length\n","- `__getitem__`: the method that defines how samples are retrieved\n","\n","Let's create a Dataset class for Iris data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYPITl8FEzOZ"},"outputs":[],"source":["import torch as tc\n","from torch.utils.data import Dataset\n","\n","class MyIrisDataset(Dataset): # Need to extend Pytorch's Dataset class\n","\n","    def __init__(self, x_data, y_data):\n","        super().__init__()\n","        # can throw in an assert to check that\n","        # inputs and labels have the same size:\n","        assert len(x_data) == len(y_data)\n","        self.x_data = tc.tensor(x_data, dtype=tc.float64)\n","        self.y_data = tc.tensor(y_data, dtype=tc.int64)\n","\n","    def __len__(self):\n","        return len(self.x_data)\n","\n","    def __getitem__(self, index):\n","        return self.x_data[index], self.y_data[index]"]},{"cell_type":"markdown","metadata":{"id":"tEegRi9dEzOZ"},"source":["Now, we can use the `MyIrisDataset` class to create the train and test dataset objects:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t94Kt7WTEzOZ","executionInfo":{"status":"ok","timestamp":1707830367402,"user_tz":-120,"elapsed":2,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fcb0bf5d-c6c5-40b5-e7ff-901e9c456218"},"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.MyIrisDataset object at 0x7fcea03cbc40>\n","<__main__.MyIrisDataset object at 0x7fcea03cb340>\n"]}],"source":["train_dataset = MyIrisDataset(x_train, y_train)\n","test_dataset = MyIrisDataset(x_test, y_test)\n","\n","print(f\"{train_dataset}\\n{test_dataset}\")"]},{"cell_type":"markdown","metadata":{"id":"xdpAK69BEzOZ"},"source":["Pytorch Datasets can be fed to `DataLoaders`, which are essentially generators that divide the data into batches of some defined size, a.k.a., the *batch size*.\n","\n","A very good and common practice is to shuffle your training data. There are many reasons for it, such as, avoiding order bias, achieving better generalization, etc.\n","\n","Apart from being a historical convention, the batch size is commonly set to be a power of 2, due to how memory is organized within the OS (caching, memory allocation, etc.), and because it is assumed for many internal optimizations to take place.   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WvANc5PEzOZ"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 2**4\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"vlkHKtZEEzOZ"},"source":["Before doing anything, let's firstly see what a batch looks like:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9JRJYU9EzOi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830367940,"user_tz":-120,"elapsed":539,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"046a2ac1-5522-425e-bb35-8d8aa7f136e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_batch shape: torch.Size([16, 4])\n","y_batch shape: torch.Size([16])\n","\n","x_batch[3:6, 0] = tensor([7.9000, 4.8000, 6.3000], dtype=torch.float64)\n","y_batch[3:6] = tensor([2, 0, 1])\n"]}],"source":["x_batch, y_batch = next(iter(train_loader))\n","\n","print(f\"x_batch shape: {x_batch.shape}\\ny_batch shape: {y_batch.shape}\")\n","print(f\"\\nx_batch[3:6, 0] = {x_batch[3:6, 0]}\\ny_batch[3:6] = {y_batch[3:6]}\")"]},{"cell_type":"markdown","metadata":{"id":"tw_jxrdtEzOi"},"source":["Usually, the number of data points is not expected to be a multiple of the batch size. Hence, the last batch will have size `len(data) % batch_size`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvtdN-w_EzOi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707830367940,"user_tz":-120,"elapsed":3,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"3fcf7947-4ab6-4228-b6d1-a3f14e62ad39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Last iteration will have a batch of size 4:\n","\n","x_batch.shape = torch.Size([16, 4])\n","x_batch.shape = torch.Size([16, 4])\n","x_batch.shape = torch.Size([16, 4])\n","x_batch.shape = torch.Size([16, 4])\n","x_batch.shape = torch.Size([16, 4])\n","x_batch.shape = torch.Size([16, 4])\n","x_batch.shape = torch.Size([4, 4])\n"]}],"source":["print(f\"Last iteration will have a batch of size {x_train.shape[0] % batch_size}:\\n\")\n","\n","for x_batch, y_batch in train_loader:\n","    print(f\"x_batch.shape = {x_batch.shape}\")\n","    # here we would feed the input batch\n","    # of this iteration into our model etc.\n"]},{"cell_type":"code","source":[],"metadata":{"id":"xY1Y8Tx2xr7O"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"hy673","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"898d28840f55b3c5c9a615fda231169adc20c90e3e87a937f55caa36837c15d9"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}