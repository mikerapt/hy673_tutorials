{"cells":[{"cell_type":"code","source":["# Comment the following lines if you're not in colab:\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# If you're in colab, cd to your own working directory here:\n","%cd ..//..//content//drive//MyDrive//Colab-Notebooks//HY-673-Tutorials//Tutorial-2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Moyb9fEn7nC","executionInfo":{"status":"ok","timestamp":1707934062449,"user_tz":-120,"elapsed":2259,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"bdda577d-a99a-49e6-e6a3-6b050667d1f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab-Notebooks/HY-673-Tutorials/Tutorial-2\n"]}]},{"cell_type":"markdown","metadata":{"id":"R0Lwhkz4QRP8"},"source":["# <u>Preliminaries</u>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZ4CIinNQRP-"},"outputs":[],"source":["import os\n","import pickle\n","import numpy as np\n","import torch as tc\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","metadata":{"id":"9Wrr1XZ_QRP_"},"source":["We can start by importing the pickled data that we made previously:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7KJ2eKgQRP_"},"outputs":[],"source":["pickledatapath = os.path.join('data', 'iris_data.pkl')\n","\n","with open(pickledatapath, 'rb') as f:\n","    x_train, x_test, y_train, y_test = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"SEkAdi9YQRP_"},"source":["Verify that all the data have the correct dimensionality:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M30VMLmYQRQA","executionInfo":{"status":"ok","timestamp":1707934064650,"user_tz":-120,"elapsed":12,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"d59b6572-97c6-4483-85fb-42f3e73d66f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_train.shape = (100, 4), y_train.shape = (100,)\n","x_test.shape  = (50, 4),  y_test.shape  = (50,)\n"]}],"source":["print(f\"x_train.shape = {x_train.shape}, y_train.shape = {y_train.shape}\")\n","print(f\"x_test.shape  = {x_test.shape},  y_test.shape  = {y_test.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"QDbONuVjQRQA"},"source":["By default, PyTorch initializes tensors as 32-bit floats, but, we can change this behavior in order for PyTorch to default to 64-bits instead. This is a tradoff between accuracy and increased memory usage and computational costs. In our case, it won't be necessary. The standard way to do so, if needed, is:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDdlw-LdQRQA"},"outputs":[],"source":["# tc.set_default_dtype(tc.float64)"]},{"cell_type":"markdown","metadata":{"id":"l-DwlqgFQRQA"},"source":["Let's set the PRNG seeds for reproducibility:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-Ta4PlDQRQA"},"outputs":[],"source":["seed = 42\n","tc.manual_seed(seed)\n","np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"oMdPYnmCQRQB"},"source":["Our dataset class from the previous notebook was:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3D1JwqlQRQB"},"outputs":[],"source":["class MyIrisDataset(Dataset):\n","\n","    def __init__(self, x_data, y_data):\n","        super().__init__()\n","        assert len(x_data) == len(y_data)\n","        self.x_data = tc.tensor(x_data, dtype=tc.float32)\n","        self.y_data = tc.tensor(y_data, dtype=tc.long)\n","\n","    def __len__(self):\n","        return len(self.x_data)\n","\n","    def __getitem__(self, index):\n","        return self.x_data[index], self.y_data[index]"]},{"cell_type":"markdown","source":["Let's check again that it works as intended:"],"metadata":{"id":"c74Mj-OezQNs"}},{"cell_type":"code","source":["batch_size = 16\n","\n","train_dataset = MyIrisDataset(x_data=x_train, y_data=y_train)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","\n","x_batch, y_batch = next(iter(train_loader))\n","print(f\"x_batch.shape = {x_batch.shape}, y_batch.shape = {y_batch.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvBMHRXYzOaE","executionInfo":{"status":"ok","timestamp":1707934064650,"user_tz":-120,"elapsed":10,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"85df4af9-c66c-4855-c217-07d9b1d5c4f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x_batch.shape = torch.Size([16, 4]), y_batch.shape = torch.Size([16])\n"]}]},{"cell_type":"markdown","metadata":{"id":"NvA1SV--QRQB"},"source":["## <u>A Very Simple Neural Network in PyTorch</u>\n","\n","A neural network model in PyTorch is an extension of the class `torch.nn.Module`. In order to define this class, we need to specify at least these 2 methods:\n","\n","- `__init__`: The constructor method. Here, we store variables, e.g., the number of features, classes, and whatever parameters are necessary for us to create a model object. Most importantly, we also create the building blocks of our model, e.g., layers, activations, etc., and define how they connect with one another. Instead of defining the entire architecture here, there is the option to use the next method (forward) to connect the layers if want. Here, we will use `Sequential` blocks, so all layers will be automatically connected in series.\n","- `forward`: Here, we declare what your model does during the forward pass given an input batch. In case of a simple sequential model, it'll just linearly pass through the architecture defined in the constructor (with a single call).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7p0pr2QAQRQB"},"outputs":[],"source":["class MyMultiLayerPerceptron(nn.Module):\n","\n","    def __init__(self, n_features, n_classes):\n","\n","        super().__init__() # initialize the attributes of the parent class\n","\n","        self.n_features = n_features\n","        self.n_classes = n_classes\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(in_features=self.n_features, out_features=5),\n","            nn.ReLU(),\n","            nn.Linear(in_features=5, out_features=self.n_classes),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"markdown","metadata":{"id":"9NX-cOKbQRQB"},"source":["To initialize a model object, we just need to create an instance of the above class. It is a helful test to just pass a batch through the network to see if we get any errors, and to see we get the expected output shape etc. Here, we have 3 classes, so we expect to get a batch of triplets for each input batch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Guyy7emOQRQB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707934064650,"user_tz":-120,"elapsed":9,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"3d54f64e-2928-4723-e801-01e68496ba2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example of this model's output before training:\n","tensor([[0.7879, 1.3486, 0.5005],\n","        [0.5511, 0.9441, 0.4546],\n","        [0.4981, 0.9760, 0.4541],\n","        [0.8637, 1.3947, 0.5281],\n","        [0.5613, 1.0368, 0.4821],\n","        [0.9687, 1.6414, 0.5729],\n","        [1.0010, 1.6669, 0.5744],\n","        [0.8306, 1.3054, 0.5114],\n","        [1.0798, 1.8091, 0.6078],\n","        [0.8793, 1.4977, 0.5331],\n","        [0.9335, 1.5683, 0.5631],\n","        [0.9781, 1.6748, 0.5800],\n","        [0.4586, 0.8746, 0.4169],\n","        [0.5203, 0.9360, 0.4530],\n","        [1.0170, 1.7536, 0.5776],\n","        [0.7678, 1.2238, 0.4867]], grad_fn=<AddmmBackward0>)\n","torch.Size([16, 3])\n"]}],"source":["model = MyMultiLayerPerceptron(n_features=4, n_classes=3)\n","\n","pred_batch = model(x_batch)\n","print(f\"Example of this model's output before training:\\n{pred_batch}\\n{pred_batch.shape}\")"]},{"cell_type":"markdown","source":["We can get a basic summary of this model's architecture simply by printing the model object:"],"metadata":{"id":"UA3WTP339g2u"}},{"cell_type":"code","source":["print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoypUGLb9cUy","executionInfo":{"status":"ok","timestamp":1707934064651,"user_tz":-120,"elapsed":10,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"95bc49a1-e172-48b1-b134-da948fddf9c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MyMultiLayerPerceptron(\n","  (model): Sequential(\n","    (0): Linear(in_features=4, out_features=5, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=5, out_features=3, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["Unfortunately, this is a very simple summary, so, in case of more complicated models, it will not be very useful. A better alternative, for instance, is to use the function `summary()` from the `tochsummary` or `torchinfo` library https://pypi.org/project/torch-summary/ (needs installation in your local environment):"],"metadata":{"id":"n3gDIna-z638"}},{"cell_type":"code","source":["from torchsummary import summary\n","summary(model=model, input_size=x_batch.shape)\n","# Warning: Calling this function requires the default tensor type to be float32.\n","# If you find a way to use it with default type float64, tell me as well."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4FbDw4hyCem","executionInfo":{"status":"ok","timestamp":1707934064651,"user_tz":-120,"elapsed":9,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"ba700b7e-2302-4706-fec6-2e9fb3bb017d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                [-1, 16, 5]              25\n","              ReLU-2                [-1, 16, 5]               0\n","            Linear-3                [-1, 16, 3]              18\n","================================================================\n","Total params: 43\n","Trainable params: 43\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.00\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"UMqpAZQ7QRQC"},"source":["Let's also show how one can get a model's trainable parameters in a more manual way without additional libraries. Firstly, we have the `model.parameters()` method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5ZHpfRzQRQC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707934064651,"user_tz":-120,"elapsed":8,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"5960812f-99f8-4eb9-a95d-e513a70da358"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object Module.parameters at 0x7d32be440510>"]},"metadata":{},"execution_count":13}],"source":["model.parameters()"]},{"cell_type":"markdown","metadata":{"id":"r2w6onIEQRQC"},"source":["This is a generator object, meaning that we can get parameters by making it an iterator and calling `next()`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghtsKvvRQRQC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707934064651,"user_tz":-120,"elapsed":7,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"ee9dc005-7ba1-46f2-8885-4dffe3545ad4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[-0.1096,  0.1009, -0.2434,  0.2936],\n","        [ 0.4408, -0.3668,  0.4346,  0.0936],\n","        [ 0.3694,  0.0677,  0.2411, -0.0706],\n","        [ 0.3854,  0.0739, -0.2334,  0.1274],\n","        [-0.2304, -0.0586, -0.2031,  0.3317]], requires_grad=True)\n","torch.Size([5, 4])\n","\n","Parameter containing:\n","tensor([-0.3947, -0.2305, -0.1412, -0.3006,  0.0472], requires_grad=True)\n","torch.Size([5])\n"]}],"source":["param_iter = iter(model.parameters())\n","\n","some_params = next(param_iter)\n","print(f\"{some_params}\\n{some_params.shape}\\n\")\n","\n","some_more_params = next(param_iter)\n","print(f\"{some_more_params}\\n{some_more_params.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"5uTJP-X2QRQC"},"source":["The first extracted parameters have shape (5, 4), meaning that they are 5 * 4 = 20 in total, and the next parameters are 5. But, that's just one set of parameters: the weights of the first linear layer are 20, and the bias parameters are 5. Hence, the first layer has a total of 20 + 5 = 25 parameters (confirmed by `torchsummary`).\n","\n","In order to get the total number of parameters of the entire model, we can just multiply the shape of each set of parameters and compute the overall sum:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGGkeqQkQRQC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707934064651,"user_tz":-120,"elapsed":6,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"fc592a36-ac5b-41e7-9656-53a44f352790"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters in our NN: 43\n"]}],"source":["n_params = np.sum([np.prod(param.shape) for param in model.parameters()])\n","print(f\"Total number of parameters in our NN: {n_params}\")"]},{"cell_type":"markdown","metadata":{"id":"Mzwpp-SLQRQC"},"source":["This number matches what `torchsummary` gave us.\n","\n","To also get the names, we can use the `model.named_parameters()` method, and print it with key-value pairs in a dictionary. Before that, however, in order to use it we need to `detatch()` it from the computational graph of the network:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6quqNbX4QRQC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707934298516,"user_tz":-120,"elapsed":312,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"1ec47381-d2d8-4483-fdca-4b1c1951efbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Layer# 0 (weight):\t 20 params\n","Layer# 0 (bias):\t  5 params\n","Layer# 2 (weight):\t 15 params\n","Layer# 2 (bias):\t  3 params\n"]}],"source":["param_dict = {name: params.detach() for name, params in model.named_parameters()}\n","\n","# If you want to print the whole dictionary:\n","# with np.printoptions(precision=2, suppress=True):\n","#    print(param_dict)\n","\n","for name, params in param_dict.items():\n","    _, layer, weight_type = name.split(\".\")\n","    print(f\"Layer# {layer} ({weight_type}):\\t{np.prod(params.shape):>3} params\")"]},{"cell_type":"markdown","source":["As you can see, our ReLU activation has been omitted since it does not introduce any parameters."],"metadata":{"id":"faMk2nft1ixk"}},{"cell_type":"markdown","metadata":{"id":"h3i8mgNHQRQC"},"source":["### <u>Additional Things Before Training</u>\n"]},{"cell_type":"markdown","metadata":{"id":"VgBdpblnQRQD"},"source":["We need to define our *loss function*. Here, we'll use cross-entropy: <br>(check theory and docs to see how the cross-entropy loss works) <br> https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j46AScBMQRQD"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","source":["Next, we need to define our *learning rate* and *optimizer*. Let us use regular (mini-batch) SGD for this simple case:"],"metadata":{"id":"rNg396Pi_cpn"}},{"cell_type":"code","source":["lr = 1e-2\n","optimizer = tc.optim.SGD(params=model.parameters(), lr=lr)"],"metadata":{"id":"fMmMEtll_Y7l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4VeehGXQRQD"},"source":["Finally, a function to evaluate our model's success. In classification problems, accuracy is a common metric, and is essentially the average number of samples that are correctly predicted by our model:\n","\n","\\begin{equation}\n","\\text{Accuracy} = \\frac{\\text{correct predictions}}{\\text{total predictions}}=\\frac{1}{N} \\sum_{i=1}^{N} 𝟙 \\{\\hat{y}_i = y_i \\},\n","\\end{equation}\n","\n","where $\\hat{y}$ are the predicted labels, $y$ are the true labels, and 𝟙 is the indicator function. <br>\n","Side note: This is a separate issue, but this metric alone does not give the entire picture of how good a classifier is. That is why other metrics, i.e, recall, precision, f1 score, are also necessary, but, we'll stick with just accuracy now for simplicity.\n","\n","The above equation can be easily computed in Python by using `np.mean(predicted_labels == true_labels)`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RW8MJ3A_QRQF"},"outputs":[],"source":["def model_accuracy(x, y):\n","    x = tc.tensor(x, dtype=tc.float32)  # cast to tensor\n","    out_tensor = model(x)               # get model's output scores\n","    out = out_tensor.detach().numpy()   # cast to numpy\n","    pred = np.argmax(out, axis=1)\n","    return np.mean(pred == y)"]},{"cell_type":"markdown","source":["If we print the training and test accuracies before training our model, we will see that its behavior is essentially equivalent to random guessing, i.e., they will both be approximately $1/3$ in our case of $3$ classes:"],"metadata":{"id":"_Kbik9J4lrtI"}},{"cell_type":"code","source":["train_acc = model_accuracy(x_train, y_train)\n","print(f\"Train accuracy before training: {train_acc}\")\n","\n","test_acc  = model_accuracy(x_test, y_test)\n","print(f\"Test accuracy before training:  {test_acc}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UEFdq6STmF-W","executionInfo":{"status":"ok","timestamp":1707934067889,"user_tz":-120,"elapsed":2,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"4070ebc6-abc0-4add-96fe-4d1bb381c8c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy before training: 0.33\n","Test accuracy before training:  0.34\n"]}]},{"cell_type":"markdown","metadata":{"id":"CmhQeVZjQRQG"},"source":["### <u>Training</u>\n","\n","Finally, time to train our model. We do that by iterating the `train_loader` for multiple *epochs*. An epoch usually means iterating through the entire training dataset once. Here's a brief summary of the steps we will follow **for each batch**:\n","\n","1) Reset the gradient stored in the optimizer with `optimizer.zero_grad()`. By default, your optimizer keeps the sums of all the gradients computed so far, so at each new iteration you normally want to clear it.\n","\n","2) Feed the batch to the model, and obtain the model's output scores, a.k.a. logits, which should be a tensor of size `(batch_size, n_classes)`. The higher a score is, the higher the model's belief is that this is the correct class.\n","\n","3) Calculate the cross-entropy loss between the logits and the true labels.\n","\n","4) Call `loss_batch.backwards()` to perform the backpropagation and compute the gradient, which is automatically added to the optimizer.\n","\n","5) Use `optimizer.step()` to update the parameters based on the current gradient. In the standard SGD, this is simply:\n","\\begin{equation}\n","w \\gets w - \\eta \\cdot \\nabla_w \\mathcal{L},\n","\\end{equation}\n","where $\\nabla_w \\mathcal{L}$ is the gradient of the loss w.r.t. the weights $w$, and $\\eta$ is the learning rate. In case one uses the *Adam* optimizer, for instance, there are some additional adaptive momentum (hence the name) terms that are taken into account, but, this is a separate topic."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HA0S541ZQRQG"},"outputs":[],"source":["n_epochs = 50\n","\n","for _ in range(n_epochs):\n","\n","    for x, y_true in train_loader:\n","\n","        optimizer.zero_grad()\n","        scores = model(x)\n","        loss = loss_fn(scores, y_true)\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"1UTtxSX-QRQG"},"source":["If everything went smoothly, our model should now have a much better accuracy. We are mainly interested in the *test accuracy*, but we should compute both to see the gap between them. That can help to determine if the model is <i>overfitting</i> the training data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKciktWqQRQG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707934069031,"user_tz":-120,"elapsed":3,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"e49e7b39-bcdc-440b-ee1a-a7bd5c7dfe1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy after training: 0.93\n","Test accuracy after training:  0.88\n"]}],"source":["train_acc = model_accuracy(x_train, y_train)\n","print(f\"Train accuracy after training: {train_acc}\")\n","\n","test_acc  = model_accuracy(x_test, y_test)\n","print(f\"Test accuracy after training:  {test_acc}\")"]},{"cell_type":"markdown","metadata":{"id":"gszPTSeHQRQG"},"source":["## <u>Homework (Optional)</u>\n","\n","Neural networks have various \"settings\", or <i>hyperparameters</i> that you can change, e.g., the learning rate, batch size, etc. You should get acquainted with them a get an idea of what role they play in the training process. Start by toying with the hyperparameters of this model and see what happens when you change:\n","- Batch size\n","- Learning rate\n","- Number of epochs\n","- Initialization (e.g., change the random seed to see how sensitive the model is to initialization)\n","- Architecture (more/less layers, bigger/smaller layers, on/off biases, other types of layers, activations, etc.)\n","- Many more (weight initialization, optimizer, regularization, etc.)"]},{"cell_type":"markdown","source":["<u>**You may want to see the pdf file of this tutorial for more details regarding everything we have said.**</u>"],"metadata":{"id":"OQIii8_G0gTg"}},{"cell_type":"code","source":[],"metadata":{"id":"WXqM51AU0iQL"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"hy673","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"898d28840f55b3c5c9a615fda231169adc20c90e3e87a937f55caa36837c15d9"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}