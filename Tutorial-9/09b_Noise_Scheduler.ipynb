{"cells":[{"cell_type":"code","source":["# # Comment the following lines if you're not in colab:\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# # If you're in colab, cd to your own working directory here:\n","# %cd ..//..//content//drive//MyDrive//Colab-Notebooks//HY-673-Tutorials//Tutorial-9"],"metadata":{"id":"p-O97E2YAXZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2N-1ZAyuAMcm"},"outputs":[],"source":["import torch as tc"]},{"cell_type":"markdown","source":["# <u>Noise Scheduler</u>\n","\n","1) When training diffusion models, we gradually apply noise to our image $x_0$, until the output is normally distributed with $\\mathcal{N}(0, 1)$. This is the task of the **noise scheduler** in a diffusion model, who dictates the amount and type of noise that should be added at each timestep $t$. It typically follows a predetermined schedule that determines how the variance of the noise increases over time, and the term *diffusion step* refers to a noise adding step in this forward deterministic process. During training, at each iteration, we apply $t$ diffusion steps, with $t$ chosen uniformly in $\\{1,2, \\dots, T\\}$, where $T$ is a predefined hyperparameter. So, given a variance $\\beta_t$ from a noise schedule $\\beta_1, \\beta_2, \\dots, \\beta_T$, the image $x$ at timestep $t$ can be described by:\n","\n","\\begin{equation}\n","x_{t} = \\sqrt{1-\\beta_t} \\cdot x_{t-1} + \\epsilon_t, \\ \\text{where} \\ \\epsilon_t \\sim \\mathcal{N}\\left(0, \\beta_t \\right), \\ \\text{or, more shortly:}\n","\\end{equation}\n","\n","\\begin{equation}\n","x_{t}\\sim \\mathcal{N}\\left(\\sqrt{1-\\beta_t} \\cdot x_{t-1}, \\ \\beta_t\\right).\n","\\end{equation}\n","\n","![](fig/fwd.png)\n","![](https://drive.google.com/uc?export=view&id=1NUpt4iIL6CeZ4M-87lAjr8QFC2vxL53C)\n","\n","2) During sampling (or generation), the diffusion model works in reverse, starting from noise and progressively removing it to generate a sample. Here, the noise scheduler specifies how to reverse the noise process. It effectively manages the variance of the noise being removed at each timestep, aiming to accurately reverse the noise addition process used during training. The scheduler guides the model with information about the noise level at each step. The model's objective during training, is to go back from $x_{t}$ to $x_{t-1}$, i.e., predict one single noise step $\\epsilon_t$. So the loss can be, for example, the MSE between the true noise $\\epsilon_t$ and the model's prediction $\\hat{\\epsilon}_{t}$.\n","\n","![](fig/bkw.png)\n","![](https://drive.google.com/uc?export=view&id=1frSbtHEK41w_oBQSKvyPC186n7uB7b0k)\n","\n","Both processes can also be defined by using Markov chains."],"metadata":{"collapsed":false,"id":"0rFTfuVGAMcp"}},{"cell_type":"markdown","source":["## <u>Problem</u>\n","\n","Applying noise to get from $x_0$ to $x_t$ is normally an iterative procedure that requires sampling noise from a Gaussian distribution $t$ times, and averaging this noise with the image.  For large enough $T$, if we actually implemented this process iteratively, training large diffusion models would be **computationally heavy** and probably even unfeasible.\n","\n","## <u>Solution</u>\n","\n","Luckily, smart researchers have come up with a method to **pre-compute** noise with **one single step**. This method computes the noise parameters at step $t$ which are the $\\bar{\\alpha}_{t}$ coefficients below. You can find more details in the slides or reading the original paper, which you can find inside the References folder. The formula to get $x_t$ directly from $x_0$ given our noise schedule $\\beta_1, \\beta_2, \\dots, \\beta_T$, is:\n","\n","\\begin{equation}\n","x_t \\sim \\mathcal{N} \\left(\\sqrt{\\bar{\\alpha}_t} \\cdot x_0, \\ 1-\\bar{\\alpha}_t\\right), \\ \\text{with:}\n","\\end{equation}\n","\\begin{equation}\n","\\alpha_t = 1 - \\beta_t, \\ \\text{and} \\ \\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s.\n","\\end{equation}\n"],"metadata":{"collapsed":false,"id":"LfsrDKP4AMcr"}},{"cell_type":"markdown","source":["It is useful to have all these parameters pre-computed in order to save extra computational costs that would be introduced during training and inference:"],"metadata":{"id":"Ua6EKcysIpq0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijO0JKVjAMcr"},"outputs":[],"source":["def get_schedules(beta_1, beta_t, timesteps):\n","    \"\"\"\n","    A linear noise scheduler for precomputing all the parameters (fractions, square roots, etc).\n","    \"\"\"\n","    beta_t = (beta_t - beta_1) * tc.arange(start=0, end=timesteps + 1, dtype=tc.float32) / timesteps + beta_1\n","    sqrt_beta_t = tc.sqrt(beta_t)\n","    alpha_t = 1 - beta_t\n","    # we'll take the logarithm of alpha_t to compute the cumulative product more stably:\n","    log_alpha_t = tc.log(alpha_t)\n","    # since we took the logarithm we exponentiate the cumulative sum:\n","    alphabar_t = tc.cumsum(log_alpha_t, dim=0).exp()\n","    sqrt_abar = tc.sqrt(alphabar_t)\n","    one_over_sqrt_a = 1 / tc.sqrt(alpha_t)\n","    sqrt_inv_abar = tc.sqrt(1 - alphabar_t)\n","    inv_abar_over_sqrt_inv_abar = (1 - alpha_t) / sqrt_inv_abar\n","    return {\n","        \"alpha\": alpha_t,\n","        \"one_over_sqrt_a\": one_over_sqrt_a,\n","        \"sqrt_beta\": sqrt_beta_t,\n","        \"alphabar\": alphabar_t,\n","        \"sqrt_abar\": sqrt_abar,\n","        \"sqrt_inv_abar\": sqrt_inv_abar,\n","        \"inv_alpha_over_sqrt_inv_abar\": inv_abar_over_sqrt_inv_abar,\n","    }"]},{"cell_type":"markdown","source":["Usage:\n","+ Define maximum allowed number of diffusion steps $n_T$.\n","+ Define $b_0$ and $b_T$."],"metadata":{"collapsed":false,"id":"Ht5IxLHXAMcs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pt78QDdzAMcs"},"outputs":[],"source":["n_T = 1000\n","betas = [1e-4, 0.02]\n","schedules = get_schedules(beta_1=betas[0], beta_t=betas[1], timesteps=n_T)"]},{"cell_type":"markdown","source":["# <u>Training </u>\n","\n","Let us simulate how we'll use the scheduler during training. From our course theory we have the following:\n","\n","![](fig/train.png)\n","![](https://drive.google.com/uc?export=view&id=1ueIRtbIhMl6PzqJpIEo_uBYJjnA8ImEn)\n","\n","### Key Remarks:\n","\n","+ It helps to rearrange the diffusion process we defined previously as to express $\\epsilon$ in terms of $x_t$ and $x_0$:\n","\\begin{equation}\n","\\epsilon = \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} \\cdot x_0} {\\sqrt{1-\\bar{\\alpha}_t}}, \\ \\epsilon \\sim \\mathcal{N}(0,1).\n","\\end{equation}\n","+ The neural network $\\epsilon_\\theta$ aims to predict the noise $\\epsilon$ that was added to the original data to get the noised version $x_t$ without knowing $x_0$. This prediction $\\hat{\\epsilon}$ will be used to iteratively denoise the data during the sampling phase, effectively reversing the diffusion process. Regarding training, all we care about is **approximating the noise** as accurately as possible.\n","+ Hence, our **loss function** will be between $\\epsilon$ and $\\hat{\\epsilon}$.\n","+ The network learns to estimate $\\epsilon$ from $x_t$ for any given $t$, which corresponds to a **specific level** of noise addition.\n","\n","### Question:\n","\n","We said that the network tries to predict the noise added in a single step, i.e., from $x_{t-1}$ to $x_{t}$, and the noise that we add in each step is different, because we have a schedule for our variances $\\beta_t$ as we mentioned. Our loss function is between $\\epsilon \\sim \\mathcal{N}(0,1)$ and the predicted noise $\\hat{\\epsilon}$, but this does not reflect the actual noise that was added in that particular step, because, $x_{t} - x_{t-1} \\in \\mathcal{N}(0, \\beta_t)$. So, how is this strategy correct for training the model?\n","\n","### Answer:\n","\n","It is effectively the same problem, because we know deterministically how to get from an $\\epsilon$ at timestep $t$ to the exact corresponding $\\epsilon_t$ (we use this transformation in the sampling/reverse process). Whether we ask from the model to output samples in $\\mathcal{N}(0,1)$ or samples $\\mathcal{N}(0,1)$ multiplied by a scalar $\\beta_t$ does not matter for our optimization problem. We actually prefer not to do that because the range we request is always the same, and it is thus numerically easier, so there is no need to overcomplicate the model's task since we're solving the same problem in the end.\n","\n","In code, and adding some more details, the training procedure can be implemented as:"],"metadata":{"collapsed":false,"id":"CROx9jzkAMcs"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def model(noised_img, diffusion_steps):\n","    # Assume code of our diffusion model with parameters theta\n","    # returning its estimation of epsilon:\n","    return tc.randn_like(noised_img)\n","\n","batch_size = 64\n","length, height = 28, 28\n","epochs = 1\n","\n","def train_loop():\n","\n","    # Step 1: Loop:\n","    for epoch in range(epochs):\n","\n","        # Step 2: Fetch a batch from the dataset (lets simulate it with random numbers):\n","        # img, _ = next(iter(dataloader)).view(-1, 1, 28, 28)\n","        x = tc.randn(size=(batch_size, 1, length, height))\n","\n","        # Step 3: Get number of timesteps t in U(1, n_T) for this iteration:\n","        timesteps = tc.randint(low=1, high=n_T + 1, size=(x.shape[0],))\n","\n","        # Step 4: Sample N(0,1) in the shape of the input:\n","        epsilon = tc.randn_like(x)\n","\n","        # Step 5: Zero gradients:\n","        # optim.zero_grad()\n","\n","        # Step 6: Add noise to the input:\n","        # sqrt(abar) * x + sqrt(1-abar) * epsilon\n","        x_t = schedules[\"sqrt_abar\"][timesteps, None, None, None] * x + \\\n","              schedules[\"sqrt_inv_abar\"][timesteps, None, None, None] * epsilon\n","\n","        # Step 7: Divide by the maximum allowed number of timesteps:\n","        t = timesteps/n_T\n","\n","        # Step 8: Get model's prediction of epsilon given the noised input x_t\n","        # and the number of diffusion steps chosen for this iteration t:\n","        epsilon_hat = model(noised_img=x_t, diffusion_steps=t)\n","\n","        # Step 9: Calculate loss between epsilon and epsilon_hat:\n","        # loss = loss_fn(eps_hat, eps)\n","\n","        # Step 10: Backpropagation:\n","        # loss.backward()\n","\n","        # Step 11: Update weights and reiterate:\n","        # optim.step()"],"metadata":{"id":"g5F3ujadAMct"}},{"cell_type":"markdown","source":["# <u>Sampling/Generation</u>\n","\n","Before closing this notebook, let us simulate the sampling process of our diffusion model. We want to progressively transform noisy data into structured output, i.e., the reverse of the diffusion process.  From our course theory, the math says:\n","\n","![](fig/sample.png)\n","![](https://drive.google.com/uc?export=view&id=1quoQs6zbP2LskAEwvffuJuqVIbiYo8SK)\n","\n","Here, $\\sigma_t$ is just our $\\beta_t$, and the rest of the notation is consistent with what we've said so far. We will straight up implement this algorithm. For more details on the matter, you can visit the course theory or the original paper."],"metadata":{"collapsed":false,"id":"Ex79BAjpAMct"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQ4g5bxDAMct"},"outputs":[],"source":["def sample(diffusion_model, timesteps, n_samples, sample_shape, schedule):\n","    # Step 1: x_t begins as N(0,1), the last diffusion step:\n","    x_T = tc.randn(size=(n_samples, *sample_shape))\n","    # Step 2: for all timesteps T:\n","    x_i = x_T\n","    for i in range(timesteps, 0, -1):\n","        # Step 3: sample z in N(0,1):\n","        z = tc.randn(size=(n_samples, *sample_shape)) if i > 1 else 0\n","        # Step 4: denoise the image to go to the previous step:\n","        ts = tc.tensor(i / timesteps).repeat(n_samples,)\n","        epsilon = diffusion_model(noised_img=x_i, diffusion_steps=ts)\n","        x_i = schedule[\"one_over_sqrt_a\"][i] * \\\n","              (x_i - schedule[\"inv_alpha_over_sqrt_inv_abar\"][i]) * epsilon + \\\n","              schedule[\"sqrt_beta\"][i] * z\n","    # Step 6: after all denoising steps, we are in the sample space:\n","    return x_i"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-Nrysz8AMct"},"outputs":[],"source":["x = sample(diffusion_model=model, timesteps=n_T, n_samples=batch_size, sample_shape=(1, 28, 28), schedule=schedules)"]},{"cell_type":"markdown","source":["## <u>Bonus: More Efficient Reverse Process</u>\n","\n","There are several techniques to accelerate the reverse process, enabling the model to require fewer steps than the forward process. This is many times done by noticing where the most effective denoising steps take place. It is beyond the scope of our tutorial, but in case you are interested on these more SOTA methods:\n","\n","+ **Learned Step Sizes:** Instead of using a fixed schedule for the noise levels, some models learn an optimal set of step sizes or intervals. This way, the model can focus computation on the most critical stages of the reverse process, skipping steps where changes are less impactful.\n","+ **Learned Variance Schedules:** Models can also learn more efficient variance schedules, which determine the amount of noise to add or remove at each step. By optimizing these schedules, the model can achieve better fidelity with fewer steps.\n","+ **Subsampling of Timesteps:** This technique involves selectively skipping certain timesteps during the reverse process. By training the model to handle larger \"jumps\" in the denoising path, fewer steps are needed to reconstruct the clean data from the noisy latent state.\n","+ **Conditioning on Multiple Steps:** Some methods involve conditioning the reverse diffusion on multiple timesteps at once, effectively teaching the model to predict several steps ahead. This approach can significantly reduce the number of necessary reverse steps.\n","+ **Using an Adaptive Solver:** In certain advanced implementations, adaptive solvers can be used to dynamically adjust the number of timesteps based on the complexity of the current reconstruction task. This means that simpler samples may require fewer steps, while more complex ones may still use more steps, optimizing overall efficiency.\n","+ **Auxiliary Networks:**  Auxiliary networks can be employed to predict larger chunks of the denoising trajectory, allowing for more substantial updates in each step. This reduces the number of steps required by improving the quality of each update.\n","\n","### <u>Example Articles</u>\n","+ https://arxiv.org/pdf/2102.09672.pdf\n","+ https://arxiv.org/pdf/2105.14080.pdf\n","+ https://arxiv.org/pdf/2206.00927.pdf\n","+ https://arxiv.org/pdf/2009.09761.pdf"],"metadata":{"collapsed":false,"id":"h5B1nTjmAMcu"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":[],"metadata":{"id":"WtVodG3BAMcu"}}],"metadata":{"kernelspec":{"display_name":"hy673","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}