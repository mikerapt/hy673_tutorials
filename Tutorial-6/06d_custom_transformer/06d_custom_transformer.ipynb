{"cells":[{"cell_type":"code","source":["# Comment the following lines if you're not in colab:\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# If you're in colab, cd to your own working directory here:\n","%cd ..//..//content//drive//MyDrive//Colab-Notebooks//HY-673-Tutorials//Tutorial-6"],"metadata":{"id":"R02zzaNNSxVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <u>Building a Transformer with PyTorch</u>\n","\n","Based on:\n","https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch <br>\n","Here, it is implemented with an extra layer normalization after the encoder and decoder to match PyTorch's implementation."],"metadata":{"collapsed":false,"id":"xCYtwLFjre-h"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["using cuda\n"]}],"source":["import torch as tc\n","import torch.nn as nn\n","from tqdm import tqdm\n","from time import perf_counter\n","from torchsummary import summary\n","\n","bar = '-'*64\n","seed = 42\n","tc.manual_seed(seed)\n","device = 'cuda' if tc.cuda.is_available() else 'cpu'\n","print(f\"using {device}\")"],"metadata":{"ExecuteTime":{"end_time":"2024-03-22T15:18:11.915210Z","start_time":"2024-03-22T15:18:10.860464900Z"},"id":"pCQirT-9re-i","executionInfo":{"status":"ok","timestamp":1711567793515,"user_tz":-120,"elapsed":2020,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6181c7ed-dbea-403b-d0da-ea0205fd6332"}},{"cell_type":"markdown","source":["### <u>Multi-Head Attention Layer</u>\n","\n","The Multi-Head Attention mechanism computes the self-attention between each pair of positions in a sequence. It consists of multiple attention heads that capture different aspects of the input sequence. Revisiting our theory document for this tutorial, we have:\n","\n","\\begin{equation}\n","  \\text{MultiHead} = \\text{Concatenate}\\left(O^{(1)}, O^{(2)}, \\dots, O^{(h)} \\right)\\cdot W_O,\n","\\end{equation}\n","\n","\\begin{equation}\n","  O^{(i)} = S^{(i)} \\cdot V^{(i)} = \\text{softmax}\\left( \\frac{Q^{(i)} \\cdot \\left(K^{(i)}\\right)^\\intercal}{\\sqrt(d_k)} \\right) \\cdot V^{(i)},\n","\\end{equation}\n","<br>\n","\\begin{equation}\n","  Q^{(i)} = W_Q^{(i)} \\cdot X, \\ K^{(i)} = W_K^{(i)} \\cdot X, \\ V^{(i)} = W_V^{(i)} \\cdot X.\n","\\end{equation}\n","\n","#### <u>Splitting and Combining for Multi-Head Attention</u>\n"," In the original Transformer model, the dimension $d_\\text{model}$ is divided among several heads, say $h$ heads. Thus, for each head, the dimensionality of $d_k$ and $d_v$ becomes $d_\\text{model}/h$. The code below simplifies the implementation by initially projecting $X$ into a space of dimension $d_\\text{model}$ for $Q,K,V$, with the understanding that this space will be subsequently divided among the heads. This is why the fully-connected layers map from $d_\\text{model}$ to $d_\\text{model}$, and the actual splitting into multiple heads (and thus into $d_k$ and $d_v$ dimensions for each head) occurs later in the code.\n","Also, in the Transformer model, each attention head uses a different set of weights to project the input $X$ into $Q,K,V$. In the code below, the weights for head are not explicitly separated in the definition of the fully-connected layers. Instead, they are defined to transform the entire input dimension $d_\\text{model}$. It is only after we apply these linear transformations that we split the resulting matrices into multiple heads with the `split_heads()` method. By this reshaping, the model implicitly uses different slices of the transformed input for each head, as if, indeed, each head has its own set of weights.\n","\n","We can implement the above operations in PyTorch like so:"],"metadata":{"collapsed":false,"id":"3iDD1RaPre-j"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"Splits the input into multiple attention heads, applies attention to each head, and then combines the results.\"\"\"\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","\n","        # Initialize dimensions:\n","        self.d_model = d_model      # Model's dimension\n","        self.num_heads = num_heads  # Number of attention heads\n","        self.d_k = tc.tensor(\n","            d_model // num_heads, device=device\n","        ) # Dimension of each head's key, query, and value\n","\n","        # Linear layers for transforming inputs:\n","        self.W_Q = nn.Linear(d_model, d_model) # Query  transformation\n","        self.W_K = nn.Linear(d_model, d_model) # Key    transformation\n","        self.W_V = nn.Linear(d_model, d_model) # Value  transformation\n","        self.W_O = nn.Linear(d_model, d_model) # Output transformation\n","\n","    def scaled_dot_product_attention(self, q, k, v, mask=None):\n","        # Calculate attention scores Q*K^T/sqrt(d_k):\n","        attn_scores = tc.matmul(q, k.transpose(-2, -1)) / tc.sqrt(self.d_k)\n","\n","        # Apply mask if provided (useful for preventing attention to certain parts like padding):\n","        if mask is not None:\n","            attn_scores = attn_scores.masked_fill(mask==0, value=-1e9)\n","\n","        # Softmax is applied row-wise to obtain the attention matrix S:\n","        s = tc.softmax(attn_scores, dim=-1)\n","\n","        # Multiply S by the values to obtain the output:\n","        return tc.matmul(s, v)\n","\n","    def split_heads(self, x):\n","        \"\"\"Reshapes the input x from\n","        (batch_size, seq_len, d_model) --> (batch_size, num_heads, seq_length, d_k).\n","        It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\"\"\"\n","        batch_size_, seq_length, d_model = x.size()\n","        return x.view(batch_size_, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n","\n","    def combine_heads(self, x):\n","        \"\"\"Reshapes the input x from\n","        (batch_size, num_heads, seq_length, d_k) --> (batch_size, seq_len, d_model).\n","        This prepares the result for further processing.\"\"\"\n","        # Combine the multiple heads back to original shape:\n","        batch_size_, _, seq_length, d_k = x.size()\n","        return x.transpose(1, 2).contiguous().view(batch_size_, seq_length, self.d_model)\n","\n","    def forward(self, q, k, v, mask=None):\n","        # Apply linear transformations and split heads:\n","        q = self.split_heads(self.W_Q(q))\n","        k = self.split_heads(self.W_K(k))\n","        v = self.split_heads(self.W_V(v))\n","        # Perform scaled dot-product (self) attention:\n","        o = self.scaled_dot_product_attention(q, k, v, mask)\n","        # Combine heads and apply output transformation:\n","        return self.W_O(self.combine_heads(o))"],"metadata":{"ExecuteTime":{"end_time":"2024-03-22T15:18:11.930051800Z","start_time":"2024-03-22T15:18:11.919226500Z"},"id":"RYAyjqK3re-j"}},{"cell_type":"markdown","source":["### <u>Position-Wise Feed-Forward Network (FFN)</u>\n","Next, let us define the Feed Forward Network (FFN) block. It is a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between:\n","\n","\\begin{equation}\n","  \\text{FFN}(X) = \\text{ReLU}\\left(X \\cdot W_1 + b1\\right)\\cdot W_2 + b2.\n","\\end{equation}\n","\n","In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs:"],"metadata":{"collapsed":false,"id":"Pbf87bsGre-k"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class PositionWiseFeedForward(nn.Module):\n","    \"\"\"Implementation of the Feed Forward Network (FFN block).\"\"\"\n","    def __init__(self, d_model, d_ff):\n","        super(PositionWiseFeedForward, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        return self.fc2(self.relu(self.fc1(x)))"],"metadata":{"ExecuteTime":{"end_time":"2024-03-22T15:18:11.947521Z","start_time":"2024-03-22T15:18:11.931553500Z"},"id":"ERDbh2rmre-k"}},{"cell_type":"markdown","source":["### <u>Positional Encoding Layer</u>\n","\n","Same as in the previous notebook:"],"metadata":{"collapsed":false,"id":"c-Hdhmk6re-k"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["loge4 = 4*tc.log(tc.tensor(10))\n","\n","class PositionalEncoding(nn.Module):\n","    \"\"\"Using sine and cosine functions of different frequencies to generate the positional encoding.\"\"\"\n","    def __init__(self, d_model, max_seq_length):\n","        super(PositionalEncoding, self).__init__()\n","        position = tc.arange(0, max_seq_length, dtype=tc.float).unsqueeze(1)\n","        div_term = tc.exp(tc.arange(0, d_model, 2).float() * -(loge4 / d_model))\n","        pe = tc.zeros(max_seq_length, d_model)\n","        pe[:, 0::2] = tc.sin(position * div_term)\n","        pe[:, 1::2] = tc.cos(position * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]"],"metadata":{"ExecuteTime":{"end_time":"2024-03-22T15:18:11.960558500Z","start_time":"2024-03-22T15:18:11.946524200Z"},"id":"nCL1y6ACre-k"}},{"cell_type":"markdown","source":["### <u>Encoder Layer</u>\n","\n","![](fig/encoder.png)\n","![](https://drive.google.com/uc?export=view&id=1YzYKw4JoiaCe5ECDrNxsfjDQOIkOT48a)\n","\n","The EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. These components together allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model:"],"metadata":{"collapsed":false,"id":"YbM1gsOtre-k"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        o = self.self_attn(q=x, k=x, v=x, mask=mask)\n","        x = self.norm1(x + self.dropout(o))\n","        ffn_output = self.feed_forward(x)\n","        return self.norm2(x + self.dropout(ffn_output))"],"metadata":{"ExecuteTime":{"end_time":"2024-03-22T15:18:11.976914600Z","start_time":"2024-03-22T15:18:11.963055800Z"},"id":"yQoDoYY5re-k"}},{"cell_type":"markdown","source":["### <u>Decoder Layer</u>\n","\n","![](fig/decoder.png)\n","![](https://drive.google.com/uc?export=view&id=1blrXGkNx_uZnXSNk34_inlyActhVGt2i)\n","\n","The DecoderLayer class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model:"],"metadata":{"collapsed":false,"id":"w4z3LeBhre-l"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_output, src_mask, tgt_mask):\n","        o = self.self_attn(q=x, k=x, v=x, mask=tgt_mask)\n","        x = self.norm1(x + self.dropout(o))\n","        o = self.cross_attn(q=x, k=enc_output, v=enc_output, mask=src_mask)\n","        x = self.norm2(x + self.dropout(o))\n","        ffn_output = self.feed_forward(x)\n","        return self.norm3(x + self.dropout(ffn_output))"],"metadata":{"ExecuteTime":{"end_time":"2024-03-22T15:18:11.989960700Z","start_time":"2024-03-22T15:18:11.977911100Z"},"id":"qcpy59ejre-l"}},{"cell_type":"markdown","source":["### <u>Transformer</u>\n","\n","![](fig/transformer.png)\n","![](https://drive.google.com/uc?export=view&id=1L-ZrIzWPkF17fnBtS0FJSCTMz5s53Tyy)\n","\n","Next, the Encoder and Decoder blocks are brought together to construct the comprehensive Transformer model. The Transformer class brings together the various components of a Transformer model, including the embeddings, positional encoding, encoder layers, and decoder layers. It provides a convenient interface for training and inference, encapsulating the complexities of multi-head attention, feed-forward networks, and layer normalization.\n","\n","### <u> Masking </u>\n","\n","The source mask (`src_mask`) in the code below is generated just for completeness; it serves no purpose in the subsequent operations here. An encoder mask would be necessary in several other scenarios, such as, Variable-Length Input Sequences with Padding, Avoiding Specific Tokens, Sequence-to-Sequence Tasks with Source Filtering, Noise Reduction, Implementing Certain Types of Attention (e.g., sparse). The target mask (`tgt_mask`), as explained in the previous notebook, is a boolean mask with `True` for unmasked and `False` for masked values.\n","\n","+ `tgt_mask` identifies which positions are real data (`True`) and which are padding (`False`).\n","+ `no_peak_mask` prevents positions from seeing future positions in the sequence (`True` means block, `False` means allow).\n","+ The `&` combines these two masks (bitwise AND). A position must be real data and not in the future to be allowed (`True` in both masks becomes `True`; any `False` in either mask becomes `False` in the combined mask)."],"metadata":{"collapsed":false,"id":"zTgEX0k4re-l"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def generate_mask(src, tgt):\n","    # y.unsqueeze(p): add an empty dimension to y at position p\n","    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n","    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n","    seq_length = tgt.size(1)\n","    no_peak_mask = (1 - tc.triu(tc.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n","    tgt_mask = tgt_mask & no_peak_mask\n","    return src_mask, tgt_mask\n","\n","class Transformer(nn.Module):\n","    def __init__(self, src_voc_size, tgt_voc_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n","        super(Transformer, self).__init__()\n","        # Same as before:\n","        self.encoder_embedding = nn.Embedding(num_embeddings=src_voc_size, embedding_dim=d_model)\n","        self.decoder_embedding = nn.Embedding(num_embeddings=tgt_voc_size, embedding_dim=d_model)\n","        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n","        # Need to add layer normalization manually now:\n","        self.enc_output_norm = nn.LayerNorm(d_model)\n","        self.dec_output_norm = nn.LayerNorm(d_model)\n","        # Encoder architecture:\n","        self.encoder_layers = nn.ModuleList(\n","            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n","        )\n","        # Decoder architecture:\n","        self.decoder_layers = nn.ModuleList(\n","            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n","        )\n","        # Same as before:\n","        self.W_O = nn.Linear(d_model, tgt_voc_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src, tgt):\n","        # Generate target mask (source mask is empty):\n","        src_mask, tgt_mask = generate_mask(src, tgt)\n","        # Same as before:\n","        src = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n","        tgt = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n","        # Pass through encoder:\n","        enc_output = src\n","        for enc_layer in self.encoder_layers:\n","            enc_output = enc_layer(enc_output, src_mask)\n","        enc_output = self.enc_output_norm(enc_output)\n","        # Pass through decoder:\n","        dec_output = tgt\n","        for dec_layer in self.decoder_layers:\n","            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n","        dec_output = self.dec_output_norm(dec_output)\n","        return self.W_O(dec_output)"],"metadata":{"ExecuteTime":{"end_time":"2024-03-22T15:18:12.008438600Z","start_time":"2024-03-22T15:18:11.994462100Z"},"id":"NTVVWFfAre-l"}},{"cell_type":"markdown","source":["Let's create an instance of our transformer model and print its architecture and trainable parameters:"],"metadata":{"collapsed":false,"id":"RzEIB77ySkQg"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Total trainable parameters = 6,481,800\n","(same as before - this is a good sign!)\n","======================================================================\n","Layer (type:depth-idx)                        Param #\n","======================================================================\n","├─Embedding: 1-1                              1,280,000\n","├─Embedding: 1-2                              1,280,000\n","├─PositionalEncoding: 1-3                     --\n","├─LayerNorm: 1-4                              512\n","├─LayerNorm: 1-5                              512\n","├─ModuleList: 1-6                             --\n","|    └─EncoderLayer: 2-1                      --\n","|    |    └─MultiHeadAttention: 3-1           263,168\n","|    |    └─PositionWiseFeedForward: 3-2      262,912\n","|    |    └─LayerNorm: 3-3                    512\n","|    |    └─LayerNorm: 3-4                    512\n","|    |    └─Dropout: 3-5                      --\n","|    └─EncoderLayer: 2-2                      --\n","|    |    └─MultiHeadAttention: 3-6           263,168\n","|    |    └─PositionWiseFeedForward: 3-7      262,912\n","|    |    └─LayerNorm: 3-8                    512\n","|    |    └─LayerNorm: 3-9                    512\n","|    |    └─Dropout: 3-10                     --\n","├─ModuleList: 1-7                             --\n","|    └─DecoderLayer: 2-3                      --\n","|    |    └─MultiHeadAttention: 3-11          263,168\n","|    |    └─MultiHeadAttention: 3-12          263,168\n","|    |    └─PositionWiseFeedForward: 3-13     262,912\n","|    |    └─LayerNorm: 3-14                   512\n","|    |    └─LayerNorm: 3-15                   512\n","|    |    └─LayerNorm: 3-16                   512\n","|    |    └─Dropout: 3-17                     --\n","|    └─DecoderLayer: 2-4                      --\n","|    |    └─MultiHeadAttention: 3-18          263,168\n","|    |    └─MultiHeadAttention: 3-19          263,168\n","|    |    └─PositionWiseFeedForward: 3-20     262,912\n","|    |    └─LayerNorm: 3-21                   512\n","|    |    └─LayerNorm: 3-22                   512\n","|    |    └─LayerNorm: 3-23                   512\n","|    |    └─Dropout: 3-24                     --\n","├─Linear: 1-8                                 1,285,000\n","├─Dropout: 1-9                                --\n","======================================================================\n","Total params: 6,481,800\n","Trainable params: 6,481,800\n","Non-trainable params: 0\n","======================================================================\n"]}],"source":["# Hyperparameters:\n","src_vocab_size = 5000\n","tgt_vocab_size = 5000\n","dmodel = 256\n","max_seq_len = 512\n","nheads = 4\n","nlayers = 2\n","dff = 512\n","drop = 0.1\n","\n","# Model instance:\n","transformer = Transformer(\n","    src_voc_size=src_vocab_size,\n","    tgt_voc_size=tgt_vocab_size,\n","    d_model=dmodel,\n","    num_heads=nheads,\n","    num_layers=nlayers,\n","    d_ff=dff,\n","    max_seq_length=max_seq_len,\n","    dropout=drop\n",").to(device)\n","\n","total_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n","print(f\"Total trainable parameters = {total_params:,}\\n(same as before - this is a good sign!)\")\n","\n","# Does not work in colab:\n","_ = summary(model=transformer)"],"metadata":{"id":"Hk7xwRUZSkQg","executionInfo":{"status":"ok","timestamp":1711567793785,"user_tz":-120,"elapsed":4,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbfa8e8b-f6ca-41ce-fa03-16b865e0cb62"}},{"cell_type":"markdown","source":["## <u>Training our Custom Transformer Model</u>\n","\n","As in previous tutorials, we can use the cross-entropy loss to train out transformer and Adam as the optimizer:"],"metadata":{"collapsed":false,"id":"2Dfl-rxUre-l"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# Training hyperparameters:\n","lr_ = 1e-4\n","betas_ = (0.9, 0.98)\n","eps_ = 1e-9\n","num_epochs = 30\n","batch_size = 32\n","\n","# Same as before:\n","loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = tc.optim.Adam(params=transformer.parameters(), lr=lr_, betas=betas_, eps=eps_)"],"metadata":{"ExecuteTime":{"end_time":"2024-03-22T15:18:12.634209700Z","start_time":"2024-03-22T15:18:12.279576400Z"},"id":"q35gyDlkre-m"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# Same as before:\n","src_data = tc.randint(1, src_vocab_size, (batch_size, max_seq_len), device=device)\n","tgt_data = tc.randint(1, tgt_vocab_size, (batch_size, max_seq_len), device=device)"],"metadata":{"id":"kMXKH6dMSkQg"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch: 30/30, Loss: 8.3157: 100%|██████████| 30/30 [00:08<00:00,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Time elapsed: 8.7087 seconds.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["tic = perf_counter()\n","# Same as before:\n","transformer.train()\n","pbar = tqdm(range(num_epochs), desc='Epochs')\n","for epoch in pbar:\n","    optimizer.zero_grad()\n","    output = transformer(src_data, tgt_data[:, :-1])\n","    output_aligned = output.contiguous().view(-1, tgt_vocab_size)\n","    target_aligned = tgt_data[:, 1:].contiguous().view(-1)\n","    loss = loss_fn(output_aligned, target_aligned)\n","    loss.backward()\n","    optimizer.step()\n","    pbar.set_description(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n","toc = perf_counter()\n","print(f\"Time elapsed: {toc-tic:.4f} seconds.\")"],"metadata":{"id":"YU87KuHGre-m","executionInfo":{"status":"error","timestamp":1711567842613,"user_tz":-120,"elapsed":44425,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"colab":{"base_uri":"https://localhost:8080/","height":373},"outputId":"5c380a9a-5156-4d4c-e1fc-057c9a8b24a5"}},{"cell_type":"markdown","source":["After or during training the model, its performance can be evaluated on a validation dataset. The following is an example of how this could be done. Likewise, `transformer.eval()` puts the transformer model in evaluation mode, which is important in order to turn off certain behaviors like dropout that are only used during training."],"metadata":{"id":"bbviMMCieFyD"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Loss: 8.6754\n"]}],"source":["# Same as before:\n","val_src_data = tc.randint(1, src_vocab_size, (batch_size, max_seq_len), device=device)\n","val_tgt_data = tc.randint(1, tgt_vocab_size, (batch_size, max_seq_len), device=device)\n","\n","transformer.eval()\n","with tc.no_grad():\n","    output = transformer(val_src_data, val_tgt_data[:, :-1])\n","    output_aligned = output.contiguous().view(-1, tgt_vocab_size)\n","    target_aligned = val_tgt_data[:, 1:].contiguous().view(-1)\n","    val_loss = loss_fn(output_aligned, target_aligned)\n","    print(f\"Validation Loss: {val_loss.item():.4f}\")"],"metadata":{"ExecuteTime":{"end_time":"2024-03-22T15:18:13.195965300Z","start_time":"2024-03-22T15:18:12.636258800Z"},"id":"BJM6ul9Dre-m","executionInfo":{"status":"aborted","timestamp":1711567842613,"user_tz":-120,"elapsed":3,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"ba58283d-a3c1-4719-ef00-0769dbb8226b"}},{"cell_type":"markdown","source":["### <u> Conclusion </u>\n","\n","In this tutorial, we demonstrated how to construct a Transformer model using PyTorch from scratch, one of the most versatile tools for deep learning. For additional begginer-level sources on the Transformer, see: <br>\n","https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face\n","\n"],"metadata":{"id":"m3TeboozhR8l"}},{"cell_type":"code","source":[],"metadata":{"id":"BOhJYL6GhuDd"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}