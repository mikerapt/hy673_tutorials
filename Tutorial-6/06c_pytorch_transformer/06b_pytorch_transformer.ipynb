{"cells":[{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["using cuda\n"]}],"source":["import torch as tc\n","from tqdm import tqdm\n","import torch.nn as nn\n","from time import perf_counter\n","from torchsummary import summary\n","\n","bar = '-'*64\n","seed = 42\n","tc.manual_seed(seed)\n","device = 'cuda' if tc.cuda.is_available() else 'cpu'\n","print(f\"using {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWxnEsfeiLxE","executionInfo":{"status":"ok","timestamp":1711552342470,"user_tz":-120,"elapsed":286,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"352744a8-5de0-4f44-a67e-861f6359ecc4"}},{"cell_type":"markdown","source":["# <u>Using a Transformer with PyTorch</u>\n","\n","With their capacity for parallelization and the ability to capture long-term dependencies in data, Transformers have immense potential in various fields, especially NLP tasks like translation, summarization, and sentiment analysis. In this notebook, we show how PyTorch's Transformer module `torch.nn.Transformer` can be used.\n","\n","![](fig/transformer.png)\n","![](https://drive.google.com/uc?export=view&id=1N_tpFwQQ2FNk4yD829q3BLOfehJtQoWo)"],"metadata":{"collapsed":false,"id":"WHpo02KKiLxG"}},{"cell_type":"markdown","source":[" ### <u>Positional Encoding Layer</u>\n","\n"," Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), the following class helps the model to consider the position of tokens in the sequence. Sinusoidal functions are used in order to allow the model to more easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence:\n","\n","\\begin{equation}\n","\\text{PE}(\\text{pos}, i) =\n","\\begin{cases}\n","\\sin\\left(f(\\text{pos}, 2i) \\right) \\\\\n","\\cos\\left(f(\\text{pos}, 2i-1) \\right)\n","\\end{cases},\n","\\end{equation}\n","where:\n","\\begin{equation}\n","  f(\\text{pos}, i) = \\text{pos} \\cdot \\exp \\left(- \\frac{4 \\cdot \\ln(10) \\cdot i}{d_\\text{model}} \\right),\n","\\end{equation}\n","and $i = 0, 1, \\dots, d_\\text{model}-1$, $\\ pos = 0, 1, \\dots, \\text{max_seq_len}-1\n","$.\n","\n","Afterwards, the positional encoding (PE) matrix is **added** (not appended) to the input (embeddings) matrix $X$, i.e., $\\text{out}_{\\text{pos}, i} = x_{\\text{pos}, i} + \\text{pe}_{\\text{pos}, i}$. This provides a sense of sequence because it introduces a unique signal at each position in the sequence. This signal is designed in such a way that the model can theoretically determine the position of each token or the distance between different tokens based on their positional encodings alone. Concatenation instead would (1) increase the dimensionality and (2) might require the model to learn to separate positional information from semantic information explicitly.\n"],"metadata":{"id":"Z4D4935TSouC"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["loge4 = 4*tc.log(tc.tensor(10))\n","\n","class PositionalEncoding(nn.Module):\n","    \"\"\"Positional Encoding using sine and cosine functions of different frequencies to generate the positional encoding.\"\"\"\n","    def __init__(self, d_model, max_seq_len):\n","        super(PositionalEncoding, self).__init__()\n","        # position = [0, 1, 2, ..., max_seq_len]:\n","        position = tc.arange(0, max_seq_len, dtype=tc.float).unsqueeze(1)\n","        # calculate for all i = [0, 2, 4, ..., d_model]:\n","        # exp((-4*ln(10)*i/d_model)):\n","        div_term = tc.exp(tc.arange(0, d_model, 2).float() * -(loge4 / d_model))\n","        # The positional encoding matrix is:\n","        pe = tc.zeros(max_seq_len, d_model)\n","        pe[:, 0::2] = tc.sin(position * div_term)\n","        pe[:, 1::2] = tc.cos(position * div_term)\n","        # self.register_buffer is used to register a tensor as a buffer in a Module.\n","        # Buffers are tensors that are not to be considered as model parameters;\n","        # i.e., they are not trainable and don't require gradients.\n","        # pe.unsqueeze(0) just adds a 0-th dimension (for the batch):\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]"],"metadata":{"id":"dZDWGzMxiLxH"}},{"cell_type":"markdown","source":["### <u>Embedding Layer</u>\n","Embeddings convert tokens (which could be words, characters, or subwords) into dense vectors of fixed size. In the case of the Transformer model, `nn.Embedding layers` are used to map each token in the source `src_voc_size` and target `tgt_voc_size` vocabularies to a high-dimensional space `d_model`. The embedding layers will transform each token into a dense vector of a fixed size, `d_model`. This means each integer in the sequence is replaced by a vector of `d_model` dimensions. Consequently, the shape of the output from the embedding layers for both src and tgt becomes `[batch_size, seq_len, d_model]`.\n","\n","An embedding layer contains a matrix of trainable parameters. For a vocabulary size of `src_vocab_size` and an embedding dimension of `d_model`, the size of this matrix is `[V, d_model]`. Each row of the matrix corresponds to the dense vector representation of a token in the vocabulary. The function for this transformation is essentially a lookup operation. Given a token ID, the embedding layer returns the corresponding row from its parameter matrix; it is like the key-value concept in a hashtable. This can be mathematically represented as a matrix multiplication where the input token IDs are used to index into the embedding matrix."],"metadata":{"id":"yWC1KXkESRWQ"}},{"cell_type":"markdown","source":["### <u> Masking </u>\n","\n","As we explained in our tutorial document, masking in Transformers is crucial for controlling the flow of information, especially in the decoder to prevent future tokens from influencing the prediction of the current token, maintaining the autoregressive property. Let us see an example:\n","\n","Suppose we have a target sequence for a language translation task: [\"Hello\", \"world\", \"!\"]. During training, the transformer model tries to predict the next word based on the previous words. The input to the decoder at each step would ideally be:\n","\n","Step 1: Input: [\"\\<bos\\>\"] Target: [\"Hello\"]\n","Step 2: Input: [\"\\<bos\\>\", \"Hello\"] Target: [\"world\"]\n","Step 3: Input: [\"\\<bos\\>\", \"Hello\", \"world\"] Target: [\"!\"]\n","\n","To ensure the model only uses past information (and Beginning of Sequence \\<bos\\> token initially) to predict the next word, we use a target mask (`tgt_mask`) that looks like this for a sequence of length $3$:\n","\n","\\begin{bmatrix}\n","  0 & -\\infty & -\\infty \\\\\n","  0 & 0 & -\\infty \\\\\n","  0 & 0 & 0\n","\\end{bmatrix}\n","This triangular matrix that we would multiply (element-wise) the input with, ensures that for each position in the sequence, the model can only attend to previous positions and itself, not future positions:\n","+ 0 means the model can attend to that position, i.e., information is visible/unmasked.\n","+ $-\\infty$ means the model cannot attend to that position, i.e., information is invisible/masked.\n","In code, that could be done as: <br>\n","`tgt_mask = tc.triu(torch.ones((3, 3)) * float('-inf'), diagonal=1)` <br>\n","A better option is to just use a very small number, e.g., $-10^{9}$, or $-10^{10}$, instead of $-\\infty$. Another option, is to use a boolean mask where `True` corresponds to the unmasked and `False` to the masked values. We will see that in the next notebook.\n","In our code below, `tgt_mask` is created using `generate_square_subsequent_mask(tgt.size(1))` because it is already part of PyTorch's `nn.Transformer` module; we do not need to manually code it. This function generates the triangular mask matrix as described, tailored to the length of the target sequence. The mask is then applied in the transformer during the forward pass to prevent the decoder from \"peeking\" at future tokens."],"metadata":{"id":"79PwU3dtXJ96"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class TransformerModel(nn.Module):\n","    def __init__(self, src_voc_size, tgt_voc_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, max_seq_len, dropout):\n","        super(TransformerModel, self).__init__()\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","        # Using PyTorch's Transformer:\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=num_heads,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=d_ff,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        # Embedding layers for encoder and decoder:\n","        self.encoder_embedding = nn.Embedding(num_embeddings=src_voc_size, embedding_dim=d_model)\n","        self.decoder_embedding = nn.Embedding(num_embeddings=tgt_voc_size, embedding_dim=d_model)\n","        # Same positional encoding for encoder & decoder (no trainable parameters):\n","        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n","        # Final linear layer:\n","        self.W_O = nn.Linear(d_model, tgt_voc_size)\n","        # Same dropout probability everywhere:\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src, tgt):\n","        # Apply embedding, then PE, then dropout, to both source and target:\n","        src = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n","        tgt = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n","        # Generate mask for the decoder:\n","        tgt_mask = self.transformer.generate_square_subsequent_mask(sz=tgt.size(1), device=device)\n","        # Apply mask and pass through the Transformer (encoder & decoder):\n","        out = self.transformer(src=src, tgt=tgt, tgt_mask=tgt_mask)\n","        # Apply final transformation (with W_O) and return:\n","        return self.W_O(out)"],"metadata":{"id":"oA0iS5ASiLxI"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Total trainable parameters = 6,481,800\n","===========================================================================\n","Layer (type:depth-idx)                             Param #\n","===========================================================================\n","├─Transformer: 1-1                                 --\n","|    └─TransformerEncoder: 2-1                     --\n","|    |    └─ModuleList: 3-1                        1,054,208\n","|    |    └─LayerNorm: 3-2                         512\n","|    └─TransformerDecoder: 2-2                     --\n","|    |    └─ModuleList: 3-3                        1,581,568\n","|    |    └─LayerNorm: 3-4                         512\n","├─Embedding: 1-2                                   1,280,000\n","├─Embedding: 1-3                                   1,280,000\n","├─PositionalEncoding: 1-4                          --\n","├─Linear: 1-5                                      1,285,000\n","├─Dropout: 1-6                                     --\n","===========================================================================\n","Total params: 6,481,800\n","Trainable params: 6,481,800\n","Non-trainable params: 0\n","===========================================================================\n"]}],"source":["# Hyperparameters:\n","src_vocab_size = 5000\n","tgt_vocab_size = 5000\n","dmodel = 256\n","max_seq_length = 512\n","nheads = 4\n","nlayers = 2\n","dff = 512\n","drop = 0.1\n","\n","# Initialize the model:\n","transformer = TransformerModel(\n","    src_voc_size=src_vocab_size,\n","    tgt_voc_size=tgt_vocab_size,\n","    d_model=dmodel,\n","    num_heads=nheads,\n","    num_encoder_layers=nlayers,\n","    num_decoder_layers=nlayers,\n","    d_ff=dff,\n","    max_seq_len=max_seq_length,\n","    dropout=drop\n",").to(device)\n","\n","total_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n","print(f\"Total trainable parameters = {total_params:,}\")\n","\n","# this does not work in colab:\n","_ = summary(model=transformer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPpXa5rKiLxI","executionInfo":{"status":"ok","timestamp":1711552716155,"user_tz":-120,"elapsed":2,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"fdfb2fb7-5a71-469b-b134-c9db937436a8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# Training hyperparameters:\n","lr_ = 1e-4\n","betas_ = (0.9, 0.98)\n","eps_ = 1e-9\n","num_epochs = 30\n","batch_size = 32\n","\n","# We can \"tell\" cross-entropy not to consider targets of a specified index.\n","# Let us chose 0 because typically it is reserved for padding tokens:\n","loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = tc.optim.Adam(params=transformer.parameters(), lr=lr_, betas=betas_, eps=eps_)"],"metadata":{"id":"iFaCy48wiLxJ"}},{"cell_type":"markdown","source":["In this tutorial, we will not use an actual dataset to train our model. We will just create a batch of random integers and just see if loss decreases or if we encounter any issues with our implementation so far.\n","\n","+ `src_data`: Random integers between 1 and `src_vocab_size`, representing a batch of source sequences with shape `(batch_size, max_seq_length)`.\n","\n","+ `tgt_data`: Random integers between 1 and tgt_vocab_size, representing a batch of target sequences with shape `(batch_size, max_seq_length)`.\n","\n","+ These random sequences can be used as inputs to the transformer model, simulating a batch of data with batch_size examples and sequences of length `max_seq_len`. This setup could be part of a larger script where the model is trained and evaluated on actual sequence-to-sequence tasks, such as machine translation or text summarization."],"metadata":{"id":"gVflV6tdiu0k"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# Random batch of data for training and validation to test if the model\n","# works so far until we find a real dataset:\n","src_data = tc.randint(1, src_vocab_size, (batch_size, max_seq_length), device=device)\n","tgt_data = tc.randint(1, tgt_vocab_size, (batch_size, max_seq_length), device=device)"],"metadata":{"id":"axa-RXGAiLxJ"}},{"cell_type":"markdown","source":["## <u>Training our PyTorch Transformer Model</u>\n","\n","What follows now is our main training loop. We have established that the model predicts the next token given the previous ones, so, we have to:\n","+ (1) Exclude the last token from the target (\"shifted right\" part) when calling the model, because essentially nothing follows afterwards for the model to predict.\n","+ (2) Exclude the first token when computing the loss because the first token the model tries to predict is not the start-of-sequence token, but the first actual token of the sequence."],"metadata":{"id":"an9aNtYzOu80"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["Epochs:   0%|          | 0/30 [00:00<?, ?it/s]C:\\Users\\miker\\anaconda3\\envs\\hy673\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n","  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n","Epoch: 30/30, Loss: 8.3667: 100%|██████████| 30/30 [00:09<00:00,  3.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Time elapsed: 9.0783 seconds.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["tic = perf_counter()\n","# Put model in training mode:\n","transformer.train()\n","# Prettier print with tqdm:\n","pbar = tqdm(range(num_epochs), desc='Epochs')\n","# Each epoch here is just iterating over the same batch:\n","for epoch in pbar:\n","    optimizer.zero_grad()\n","    # Exclude the last token from target (\"shifted right\" part):\n","    output = transformer(src_data, tgt_data[:, :-1])\n","    # [batch_size, seq_len, vocab_size] -> [batch_size * seq_len, vocab_size]:\n","    output_aligned = output.contiguous().view(-1, tgt_vocab_size)\n","    # Exclude the first token:\n","    target_aligned = tgt_data[:, 1:].contiguous().view(-1)\n","    # Calculate loss:\n","    loss = loss_fn(output_aligned, target_aligned)\n","    # Backpropagation:\n","    loss.backward()\n","    # Update weights:\n","    optimizer.step()\n","    # Print:\n","    pbar.set_description(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n","toc = perf_counter()\n","print(f\"Time elapsed: {toc-tic:.4f} seconds.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"3gdD56-oiLxJ","executionInfo":{"status":"error","timestamp":1711552370226,"user_tz":-120,"elapsed":27071,"user":{"displayName":"Michail Raptakis","userId":"13076296806103248309"}},"outputId":"8fab93c8-d12e-48dd-da0a-c5f2dddba612"}},{"cell_type":"markdown","source":["Let's also simulate how one could evalute this model using a validation batch. This could be done during training to find an early-stop criterion and to adjust the hyperparameters better:"],"metadata":{"id":"r4hv-VQjRYie"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Loss: 8.6479\n"]}],"source":["# Validation batch:\n","val_src_data = tc.randint(1, src_vocab_size, (batch_size, max_seq_length), device=device)\n","val_tgt_data = tc.randint(1, tgt_vocab_size, (batch_size, max_seq_length), device=device)\n","\n","# Put model in evaluation mode:\n","transformer.eval()\n","# Same commands:\n","with tc.no_grad():\n","    output = transformer(val_src_data, val_tgt_data[:, :-1])\n","    output_aligned = output.contiguous().view(-1, tgt_vocab_size)\n","    target_aligned = val_tgt_data[:, 1:].contiguous().view(-1)\n","    val_loss = loss_fn(output_aligned, target_aligned)\n","    print(f\"Validation Loss: {val_loss.item():.4f}\")"],"metadata":{"id":"HFsGb6xAiLxJ","outputId":"8a19d880-877d-44a9-a1eb-666063f6183a"}},{"cell_type":"code","source":[],"metadata":{"id":"jQUvAFzbTdgL"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}